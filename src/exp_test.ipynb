{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..running\n",
      "Epoch: 0, train nll=3728.421875\n",
      "val nll=1863.9881041666667\n",
      "saved!\n",
      "Epoch: 1, train nll=3706.7265625\n",
      "val nll=1853.1076666666668\n",
      "saved!\n",
      "Epoch: 2, train nll=3684.103759765625\n",
      "val nll=1841.9745\n",
      "saved!\n",
      "Epoch: 3, train nll=3661.23193359375\n",
      "val nll=1830.4989375\n",
      "saved!\n",
      "Epoch: 4, train nll=3637.6728515625\n",
      "val nll=1818.72975\n",
      "saved!\n",
      "Epoch: 5, train nll=3613.4111328125\n",
      "val nll=1806.5425\n",
      "saved!\n",
      "Epoch: 6, train nll=3588.4775390625\n",
      "val nll=1794.0138333333334\n",
      "saved!\n",
      "Epoch: 7, train nll=3562.36328125\n",
      "val nll=1781.0469166666667\n",
      "saved!\n",
      "Epoch: 8, train nll=3536.0390625\n",
      "val nll=1767.7661458333334\n",
      "saved!\n",
      "Epoch: 9, train nll=3511.66552734375\n",
      "val nll=1754.1437916666666\n",
      "saved!\n",
      "FINAL LOSS: nll=1754.1552125\n"
     ]
    }
   ],
   "source": [
    "print('..running')\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pytorch_model_summary import summary\n",
    "import yaml\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "from util import samples_generated, samples_real, plot_curve\n",
    "import idf\n",
    "from train import evaluation, training \n",
    "from data import load_data\n",
    "from neural_networks import nnetts\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size = 1000\n",
    "train_data, val_data, test_data = load_data('mnist')\n",
    "# Create data loaders\n",
    "training_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "result_dir = 'results/exp_test'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "name = 'idf-4'\n",
    "\n",
    "D = 784   # input dimension\n",
    "M = D  # the number of neurons in scale (s) and translation (t) nets\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 10 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped\n",
    "num_flows = 4 # The number of invertible transformations\n",
    "lam = 0. # Regularization Hyperparameter\n",
    "\n",
    "hyperparameters = {'D': D, \n",
    "                   'M': M,\n",
    "                   'lr': lr,\n",
    "                   'num_epochs': num_epochs,\n",
    "                   'max_patience': max_patience,\n",
    "                   'num_flows': num_flows,\n",
    "                   'batch_size': batch_size,\n",
    "                   'lambda': lam\n",
    "                    }\n",
    "\n",
    "with open(result_dir + '/hyperparameters.yaml', 'w') as file:\n",
    "    yaml.dump(hyperparameters, file)\n",
    "\n",
    "netts = nnetts(D, M)\n",
    "model = idf.IDF4(netts, num_flows, D=D).to(device)\n",
    "#print(summary(model, torch.zeros(1, 64), show_input=False, show_hierarchical=False))\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)\n",
    "# Training procedure\n",
    "nll_val = training(name=name, result_dir = result_dir, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                       training_loader=training_loader, val_loader=val_loader, device=device, lam=lam)\n",
    "\n",
    "with open(result_dir + '/train_loss.txt', \"w\") as file:\n",
    "    for item in nll_val:\n",
    "        file.write(f\"{item}\\n\")\n",
    "\n",
    "test_loss = evaluation(name=result_dir + '/' + name, test_loader=test_loader)\n",
    "f = open(result_dir + '/test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "samples_generated(result_dir + '/' + name, test_loader, 28)\n",
    "plot_curve(result_dir + '/' + name, nll_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
