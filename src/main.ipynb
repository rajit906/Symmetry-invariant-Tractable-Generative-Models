{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/jmtomczak/git_flow/blob/main/models/idf.py\n",
    "# https://arxiv.org/pdf/2011.15056\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from pytorch_model_summary import summary\n",
    "import yaml\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to flatten the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "    transforms.Lambda(lambda x: x.view(-1))  # Flatten the image\n",
    "])\n",
    "\n",
    "# Custom dataset class to return only images\n",
    "class MNISTWithoutLabels(datasets.MNIST):\n",
    "    def __getitem__(self, index):\n",
    "        img, _ = super().__getitem__(index)  # Ignore the label\n",
    "        return img\n",
    "\n",
    "# Download and load the training data\n",
    "train_dataset = MNISTWithoutLabels(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Download and load the test data\n",
    "test_dataset = MNISTWithoutLabels(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Define the validation split size\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Create data loaders\n",
    "training_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we go wild and use a dataset that is simpler than MNIST! We use a scipy dataset called Digits. It consists of ~1500 images of size 8x8, and each pixel can take values in $\\{0, 1, \\ldots, 16\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import samples_generated, samples_real, plot_curve\n",
    "import idf\n",
    "from train import evaluation, training \n",
    "from data import load_data\n",
    "from neural_networks import nnetts\n",
    "\n",
    "#train_data, val_data, test_data = load_data(name = 'sklearn')\n",
    "#training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "#test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "result_dir = 'results/exp_1'\n",
    "if not(os.path.exists(result_dir)):\n",
    "    os.mkdir(result_dir)\n",
    "name = 'idf'\n",
    "\n",
    "D = 784   # input dimension\n",
    "M = 784  # the number of neurons in scale (s) and translation (t) nets\n",
    "lr = 1e-3 # learning rate\n",
    "num_epochs = 100 # max. number of epochs\n",
    "max_patience = 20 # an early stopping is used, if training doesn't improve for longer than 20 epochs, it is stopped\n",
    "num_flows = 4 # The number of invertible transformations\n",
    "\n",
    "hyperparameters = {'D': D, \n",
    "                   'M': M,\n",
    "                   'lr': lr,\n",
    "                   'num_epochs': num_epochs,\n",
    "                   'max_patience': max_patience,\n",
    "                   'num_flows': num_flows,\n",
    "                   'batch_size': batch_size\n",
    "                    }\n",
    "\n",
    "with open(result_dir + '/hyperparameters.yaml', 'w') as file:\n",
    "    yaml.dump(hyperparameters, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF by JT.\n"
     ]
    }
   ],
   "source": [
    "netts = nnetts(D, M)\n",
    "# Init IDF\n",
    "model = idf.IDF(netts, num_flows, D=D)\n",
    "# Print the summary (like in Keras)\n",
    "#print(summary(model, torch.zeros(1, 64), show_input=False, show_hierarchical=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "optimizer = torch.optim.Adamax([p for p in model.parameters() if p.requires_grad == True], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, val nll=1842.3470208333333\n",
      "saved!\n",
      "Epoch: 1, val nll=1801.054625\n",
      "saved!\n",
      "Epoch: 2, val nll=1760.0371458333334\n",
      "saved!\n",
      "Epoch: 3, val nll=1719.0781666666667\n",
      "saved!\n",
      "Epoch: 4, val nll=1677.7449375\n",
      "saved!\n",
      "Epoch: 5, val nll=1637.5582291666667\n",
      "saved!\n",
      "Epoch: 6, val nll=1597.37925\n",
      "saved!\n",
      "Epoch: 7, val nll=1558.2645416666667\n",
      "saved!\n",
      "Epoch: 8, val nll=1518.6429166666667\n",
      "saved!\n",
      "Epoch: 9, val nll=1479.850875\n",
      "saved!\n",
      "Epoch: 10, val nll=1441.0365625\n",
      "saved!\n",
      "Epoch: 11, val nll=1403.1942708333333\n",
      "saved!\n",
      "Epoch: 12, val nll=1364.7166875\n",
      "saved!\n",
      "Epoch: 13, val nll=1326.5913333333333\n",
      "saved!\n",
      "Epoch: 14, val nll=1290.3303333333333\n",
      "saved!\n",
      "Epoch: 15, val nll=1255.7547083333334\n",
      "saved!\n",
      "Epoch: 16, val nll=1225.3941458333334\n",
      "saved!\n",
      "Epoch: 17, val nll=1187.9784375\n",
      "saved!\n",
      "Epoch: 18, val nll=1156.7870833333334\n",
      "saved!\n",
      "Epoch: 19, val nll=1124.9948541666668\n",
      "saved!\n",
      "Epoch: 20, val nll=1092.7890833333333\n",
      "saved!\n",
      "Epoch: 21, val nll=1067.6906458333333\n",
      "saved!\n",
      "Epoch: 22, val nll=1041.75621875\n",
      "saved!\n",
      "Epoch: 23, val nll=1016.4135\n",
      "saved!\n",
      "Epoch: 24, val nll=979.3858958333333\n",
      "saved!\n",
      "Epoch: 25, val nll=957.7770520833334\n",
      "saved!\n",
      "Epoch: 26, val nll=929.3645729166667\n",
      "saved!\n",
      "Epoch: 27, val nll=905.3243333333334\n",
      "saved!\n",
      "Epoch: 28, val nll=896.95771875\n",
      "saved!\n",
      "Epoch: 29, val nll=870.3080729166667\n",
      "saved!\n",
      "Epoch: 30, val nll=860.0118645833334\n",
      "saved!\n",
      "Epoch: 31, val nll=858.2010625\n",
      "saved!\n",
      "Epoch: 32, val nll=820.0584791666666\n",
      "saved!\n",
      "Epoch: 33, val nll=825.7297083333333\n",
      "Epoch: 34, val nll=797.6245520833334\n",
      "saved!\n",
      "Epoch: 35, val nll=819.0233125\n",
      "Epoch: 36, val nll=776.6618541666667\n",
      "saved!\n",
      "Epoch: 37, val nll=799.65275\n",
      "Epoch: 38, val nll=771.8972395833333\n",
      "saved!\n",
      "Epoch: 39, val nll=766.6191354166667\n",
      "saved!\n",
      "Epoch: 40, val nll=782.5302708333334\n",
      "Epoch: 41, val nll=751.6005520833334\n",
      "saved!\n",
      "Epoch: 42, val nll=726.5264375\n",
      "saved!\n",
      "Epoch: 43, val nll=757.3416354166667\n",
      "Epoch: 44, val nll=765.9385833333333\n",
      "Epoch: 45, val nll=766.69615625\n",
      "Epoch: 46, val nll=785.4321770833334\n",
      "Epoch: 47, val nll=740.2338958333333\n",
      "Epoch: 48, val nll=775.6261145833333\n",
      "Epoch: 49, val nll=763.5181145833334\n",
      "Epoch: 50, val nll=761.7207083333333\n",
      "Epoch: 51, val nll=726.8047291666667\n",
      "Epoch: 52, val nll=762.9710104166667\n",
      "Epoch: 53, val nll=762.0490833333333\n",
      "Epoch: 54, val nll=779.4870833333333\n",
      "Epoch: 55, val nll=816.0253541666667\n",
      "Epoch: 56, val nll=785.886875\n",
      "Epoch: 57, val nll=801.3621770833333\n",
      "Epoch: 58, val nll=768.5783229166667\n",
      "Epoch: 59, val nll=724.735875\n",
      "saved!\n",
      "Epoch: 60, val nll=751.2700625\n",
      "Epoch: 61, val nll=792.8314166666667\n",
      "Epoch: 62, val nll=787.1407916666667\n",
      "Epoch: 63, val nll=796.5665729166667\n",
      "Epoch: 64, val nll=764.7429895833334\n",
      "Epoch: 65, val nll=776.1041875\n",
      "Epoch: 66, val nll=790.9424895833333\n",
      "Epoch: 67, val nll=727.8751458333334\n",
      "Epoch: 68, val nll=746.1414895833333\n",
      "Epoch: 69, val nll=711.14790625\n",
      "saved!\n",
      "Epoch: 70, val nll=738.7619583333334\n",
      "Epoch: 71, val nll=738.0984375\n",
      "Epoch: 72, val nll=694.60309375\n",
      "saved!\n",
      "Epoch: 73, val nll=775.1492395833334\n",
      "Epoch: 74, val nll=730.40909375\n",
      "Epoch: 75, val nll=793.5206458333333\n",
      "Epoch: 76, val nll=763.7436666666666\n",
      "Epoch: 77, val nll=826.1948854166667\n",
      "Epoch: 78, val nll=778.14028125\n",
      "Epoch: 79, val nll=734.8248645833334\n",
      "Epoch: 80, val nll=766.46390625\n",
      "Epoch: 81, val nll=750.3498229166667\n",
      "Epoch: 82, val nll=718.5523333333333\n",
      "Epoch: 83, val nll=733.4326458333334\n",
      "Epoch: 84, val nll=745.3168020833333\n",
      "Epoch: 85, val nll=761.8679375\n",
      "Epoch: 86, val nll=755.6253645833333\n",
      "Epoch: 87, val nll=796.6243020833333\n",
      "Epoch: 88, val nll=710.6816979166666\n",
      "Epoch: 89, val nll=755.21875\n",
      "Epoch: 90, val nll=758.8820729166666\n",
      "Epoch: 91, val nll=720.2804583333333\n",
      "Epoch: 92, val nll=737.3903958333333\n",
      "Epoch: 93, val nll=790.1799791666666\n"
     ]
    }
   ],
   "source": [
    "# Training procedure\n",
    "nll_val = training(name=result_dir + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer,\n",
    "                       training_loader=training_loader, val_loader=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL LOSS: nll=695.8311625\n"
     ]
    }
   ],
   "source": [
    "with open(result_dir + '/train_loss.txt', \"w\") as file:\n",
    "    for item in nll_val:\n",
    "        file.write(f\"{item}\\n\")\n",
    "\n",
    "test_loss = evaluation(name=result_dir + '/' + name, test_loader=test_loader)\n",
    "f = open(result_dir + '/test_loss.txt', \"w\")\n",
    "f.write(str(test_loss))\n",
    "f.close()\n",
    "\n",
    "samples_generated(result_dir + '/' + name, test_loader, 28)\n",
    "plot_curve(result_dir + '/' + name, nll_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
